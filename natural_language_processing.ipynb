{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOLx3HCKIucN1mBVPaQb8qg",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/KarishmaSaxena/naturalLanguageProcessing/blob/main/natural_language_processing.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "corpus - parapraph\n",
        "\n",
        "documnet - sentences\n",
        "\n",
        "vocabulary - unique words\n",
        "\n",
        "words - all words present in corpus\n",
        "\n",
        "\n",
        "tokenization -> paragrapgh->process(pre)->{generating -> sentences, words from paragraph}-> into tokens\n"
      ],
      "metadata": {
        "id": "vQWlkaouFR34"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Iuh8PwUgCSNX",
        "outputId": "18f5c00d-42ca-42bf-99c2-49a197aebecc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.12/dist-packages (3.9.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.12/dist-packages (from nltk) (8.3.1)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.12/dist-packages (from nltk) (1.5.3)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.12/dist-packages (from nltk) (2025.11.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from nltk) (4.67.1)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 75
        }
      ],
      "source": [
        "!pip install nltk\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('punkt_tab')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "corpus =\"Hello Welcome , I am Karishma Saxena and I am learning NLP . NLP helps machines understand human language. I enjoy working on AI and machine learning projects. I practice Python and machine learning daily. My goal is to build intelligent applications.\""
      ],
      "metadata": {
        "id": "jDJ4oVdeCrD-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(corpus)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0U6Xu5DOD79g",
        "outputId": "28bcb79d-8241-4a47-9187-49af71c7ad14"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hello Welcome , I am Karishma Saxena and I am learning NLP . NLP helps machines understand human language. I enjoy working on AI and machine learning projects. I practice Python and machine learning daily. My goal is to build intelligent applications.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Tokenization\n",
        "#paragraph->> sentences\n",
        "from nltk.tokenize import sent_tokenize"
      ],
      "metadata": {
        "id": "OA3SvPonCrv7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "documnets = sent_tokenize(corpus)"
      ],
      "metadata": {
        "id": "QQfvWLekCsBo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "type(documnets)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7mmv5w_dCsQy",
        "outputId": "1404ce69-c105-4b04-c0f1-8c891aa991c7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "list"
            ]
          },
          "metadata": {},
          "execution_count": 80
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#itreatting over documnets / sentences\n",
        "for sentences in documnets:\n",
        "  print(sentences)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cKUHElyPEtzv",
        "outputId": "deb87f40-1df3-4d3c-ce46-34217cb04378"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hello Welcome , I am Karishma Saxena and I am learning NLP .\n",
            "NLP helps machines understand human language.\n",
            "I enjoy working on AI and machine learning projects.\n",
            "I practice Python and machine learning daily.\n",
            "My goal is to build intelligent applications.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Tokenization\n",
        "# para->>>words\n",
        "from nltk.tokenize import word_tokenize\n",
        "word_tokenize(corpus)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0q9Y_Xb4EuO5",
        "outputId": "464753c4-8f23-4a49-cc80-bc0e52dabce1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Hello',\n",
              " 'Welcome',\n",
              " ',',\n",
              " 'I',\n",
              " 'am',\n",
              " 'Karishma',\n",
              " 'Saxena',\n",
              " 'and',\n",
              " 'I',\n",
              " 'am',\n",
              " 'learning',\n",
              " 'NLP',\n",
              " '.',\n",
              " 'NLP',\n",
              " 'helps',\n",
              " 'machines',\n",
              " 'understand',\n",
              " 'human',\n",
              " 'language',\n",
              " '.',\n",
              " 'I',\n",
              " 'enjoy',\n",
              " 'working',\n",
              " 'on',\n",
              " 'AI',\n",
              " 'and',\n",
              " 'machine',\n",
              " 'learning',\n",
              " 'projects',\n",
              " '.',\n",
              " 'I',\n",
              " 'practice',\n",
              " 'Python',\n",
              " 'and',\n",
              " 'machine',\n",
              " 'learning',\n",
              " 'daily',\n",
              " '.',\n",
              " 'My',\n",
              " 'goal',\n",
              " 'is',\n",
              " 'to',\n",
              " 'build',\n",
              " 'intelligent',\n",
              " 'applications',\n",
              " '.']"
            ]
          },
          "metadata": {},
          "execution_count": 82
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#sentences->> words\n",
        "for sentences in documnets:\n",
        "  print(word_tokenize(sentences))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ukk4FiBzGN5U",
        "outputId": "0b727ec6-647d-409f-8da0-e6597cebe82d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Hello', 'Welcome', ',', 'I', 'am', 'Karishma', 'Saxena', 'and', 'I', 'am', 'learning', 'NLP', '.']\n",
            "['NLP', 'helps', 'machines', 'understand', 'human', 'language', '.']\n",
            "['I', 'enjoy', 'working', 'on', 'AI', 'and', 'machine', 'learning', 'projects', '.']\n",
            "['I', 'practice', 'Python', 'and', 'machine', 'learning', 'daily', '.']\n",
            "['My', 'goal', 'is', 'to', 'build', 'intelligent', 'applications', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# also separates puncations\n",
        "from nltk.tokenize import wordpunct_tokenize\n",
        "print(wordpunct_tokenize(corpus))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YyKbpI3cGYQv",
        "outputId": "040c5543-01fc-42ff-c7af-f0dc3663417f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Hello', 'Welcome', ',', 'I', 'am', 'Karishma', 'Saxena', 'and', 'I', 'am', 'learning', 'NLP', '.', 'NLP', 'helps', 'machines', 'understand', 'human', 'language', '.', 'I', 'enjoy', 'working', 'on', 'AI', 'and', 'machine', 'learning', 'projects', '.', 'I', 'practice', 'Python', 'and', 'machine', 'learning', 'daily', '.', 'My', 'goal', 'is', 'to', 'build', 'intelligent', 'applications', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#\n",
        "from nltk.tokenize import TreebankWordTokenizer\n",
        "word_tokenize(corpus)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "81aWFdPrGt3a",
        "outputId": "cd3d1ab2-0a1b-42d3-8ed2-a660f1b82d8c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Hello',\n",
              " 'Welcome',\n",
              " ',',\n",
              " 'I',\n",
              " 'am',\n",
              " 'Karishma',\n",
              " 'Saxena',\n",
              " 'and',\n",
              " 'I',\n",
              " 'am',\n",
              " 'learning',\n",
              " 'NLP',\n",
              " '.',\n",
              " 'NLP',\n",
              " 'helps',\n",
              " 'machines',\n",
              " 'understand',\n",
              " 'human',\n",
              " 'language',\n",
              " '.',\n",
              " 'I',\n",
              " 'enjoy',\n",
              " 'working',\n",
              " 'on',\n",
              " 'AI',\n",
              " 'and',\n",
              " 'machine',\n",
              " 'learning',\n",
              " 'projects',\n",
              " '.',\n",
              " 'I',\n",
              " 'practice',\n",
              " 'Python',\n",
              " 'and',\n",
              " 'machine',\n",
              " 'learning',\n",
              " 'daily',\n",
              " '.',\n",
              " 'My',\n",
              " 'goal',\n",
              " 'is',\n",
              " 'to',\n",
              " 'build',\n",
              " 'intelligent',\n",
              " 'applications',\n",
              " '.']"
            ]
          },
          "metadata": {},
          "execution_count": 85
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Stemming\n",
        "\n",
        "Stemming is an NLP text preprocessing technique.\n",
        "\n",
        "It reduces words to their root/base form (called stem).\n",
        "\n",
        "The stem may not be a valid dictionary word.\n",
        "\n",
        "Used to normalize text so similar words are treated the same.\n",
        "\n",
        "Example:\n",
        "\n",
        "playing, played, plays → play\n",
        "\n",
        "studies, studying → studi"
      ],
      "metadata": {
        "id": "eOGnXZ1WjmqN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#stemming\n",
        "from nltk.stem import PorterStemmer\n",
        "word = [\"learning\",\"learned\",\"studies\",\"studying\",\"runner\",\"running\",\"connection\",\"connected\",\"easily\",\"fairness\",\"eat\",\"eates\",\"eaten\",\"eating\"]\n"
      ],
      "metadata": {
        "id": "OtlTU2utHQz_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#PorterStemmer\n",
        "stemming = PorterStemmer()"
      ],
      "metadata": {
        "id": "KqWpffAUIwK2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for w in word:\n",
        "    print(w+ \"------->\"+ stemming.stem(w))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5_0UDlb6I85q",
        "outputId": "3eca054c-cd0d-4ec0-fbe8-73f26ddfe9d5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "learning------->learn\n",
            "learned------->learn\n",
            "studies------->studi\n",
            "studying------->studi\n",
            "runner------->runner\n",
            "running------->run\n",
            "connection------->connect\n",
            "connected------->connect\n",
            "easily------->easili\n",
            "fairness------->fair\n",
            "eat------->eat\n",
            "eates------->eat\n",
            "eaten------->eaten\n",
            "eating------->eat\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "stemming.stem('congratulations')"
      ],
      "metadata": {
        "id": "o6jl4gLmLLlC",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "5baf4bd7-f22c-4c09-873d-65c38e4cfcb5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'congratul'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 89
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import SnowballStemmer\n",
        "snow_stemmer = SnowballStemmer(language='english')"
      ],
      "metadata": {
        "id": "18j5dmYLjPvk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for word in word:\n",
        "   print(snow_stemmer.stem(word))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eP0pvdEfkiA-",
        "outputId": "d14bbe41-bf97-46da-cc54-18436810f02b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "learn\n",
            "learn\n",
            "studi\n",
            "studi\n",
            "runner\n",
            "run\n",
            "connect\n",
            "connect\n",
            "easili\n",
            "fair\n",
            "eat\n",
            "eat\n",
            "eaten\n",
            "eat\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# potter stemmer\n",
        "stemming.stem('fairly'), stemming.stem('sportingly')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fFK0GazymL0d",
        "outputId": "0f585cfd-a49e-4eff-b61b-2116df1d09b5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('fairli', 'sportingli')"
            ]
          },
          "metadata": {},
          "execution_count": 97
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# same thing with snowball\n",
        "snow_stemmer.stem('fairly'),snow_stemmer.stem('sportingly')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ydZTZ-NCmnzq",
        "outputId": "bfece089-98d9-4ef7-aa96-8532ece7a2dc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('fair', 'sport')"
            ]
          },
          "metadata": {},
          "execution_count": 98
        }
      ]
    }
  ]
}