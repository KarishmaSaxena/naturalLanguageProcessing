{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMmgsFBv5GtKmBQDLUBCBuU",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/KarishmaSaxena/naturalLanguageProcessing/blob/main/Text_Preprocessing_Stopwords.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! pip install nltk\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt_tab')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ji2yLTlv5IHD",
        "outputId": "6fd8534f-99ed-4e7c-9338-628400e644eb"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.12/dist-packages (3.9.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.12/dist-packages (from nltk) (8.3.1)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.12/dist-packages (from nltk) (1.5.3)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.12/dist-packages (from nltk) (2025.11.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from nltk) (4.67.1)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "N0CLjDWH4BE3"
      },
      "outputs": [],
      "source": [
        "paragraph = '''\n",
        "You are tired, and that is not weakness.\n",
        "It means you’ve been trying.\n",
        "It means you’ve been showing up when it would’ve been easier to disappear.\n",
        "Progress doesn’t always look like results.\n",
        "Sometimes it looks like discipline without applause.\n",
        "\n",
        "There will be days when effort gives nothing back.\n",
        "Days when studying feels heavy and pointless.\n",
        "Days when motivation is silent.\n",
        "On those days, remember this: consistency matters more than intensity.\n",
        "\n",
        "You don’t need to give 100%.\n",
        "You need to give *something*.\n",
        "Even 20 minutes counts.\n",
        "Even one page counts.\n",
        "Even one problem solved slowly counts.\n",
        "\n",
        "Success is built in boring moments.\n",
        "In quiet rooms.\n",
        "In tired eyes.\n",
        "In “I’ll try anyway” energy.\n",
        "\n",
        "You are not behind.\n",
        "You are in training.\n",
        "Every small session is a vote for the person you are becoming.\n",
        "Every time you sit down despite resistance, you win.\n",
        "\n",
        "Results lag behind effort.\n",
        "Always.\n",
        "But they never appear without it.\n",
        "\n",
        "Don’t quit because today was hard.\n",
        "Don’t quit because yesterday was empty.\n",
        "Don’t quit because the future is unclear.\n",
        "\n",
        "Show up.\n",
        "Sit down.\n",
        "Breathe.\n",
        "Do what you can.\n",
        "\n",
        "20 minutes today becomes confidence tomorrow.\n",
        "Confidence becomes momentum.\n",
        "Momentum becomes change.\n",
        "\n",
        "You don’t need proof yet.\n",
        "You need patience.\n",
        "\n",
        "Keep going.\n",
        "Not because it’s easy—\n",
        "but because quitting won’t take you where you want to be.\n",
        "\n",
        "'''"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Stop words = words we often remove because they don’t help much in understanding the main content.\n",
        "\n",
        "These words occur in almost every sentence, so:\n",
        "\n",
        "They don’t help differentiate documents\n",
        "\n",
        "They increase noise\n",
        "\n",
        "They slow down processing"
      ],
      "metadata": {
        "id": "uFJTI7Ci5Jiz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.corpus import stopwords\n",
        "stopwords.words('english')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xSbVfZ9Z5gDn",
        "outputId": "c90c6705-6b47-46f0-8a02-c9de60aed1a2"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['a',\n",
              " 'about',\n",
              " 'above',\n",
              " 'after',\n",
              " 'again',\n",
              " 'against',\n",
              " 'ain',\n",
              " 'all',\n",
              " 'am',\n",
              " 'an',\n",
              " 'and',\n",
              " 'any',\n",
              " 'are',\n",
              " 'aren',\n",
              " \"aren't\",\n",
              " 'as',\n",
              " 'at',\n",
              " 'be',\n",
              " 'because',\n",
              " 'been',\n",
              " 'before',\n",
              " 'being',\n",
              " 'below',\n",
              " 'between',\n",
              " 'both',\n",
              " 'but',\n",
              " 'by',\n",
              " 'can',\n",
              " 'couldn',\n",
              " \"couldn't\",\n",
              " 'd',\n",
              " 'did',\n",
              " 'didn',\n",
              " \"didn't\",\n",
              " 'do',\n",
              " 'does',\n",
              " 'doesn',\n",
              " \"doesn't\",\n",
              " 'doing',\n",
              " 'don',\n",
              " \"don't\",\n",
              " 'down',\n",
              " 'during',\n",
              " 'each',\n",
              " 'few',\n",
              " 'for',\n",
              " 'from',\n",
              " 'further',\n",
              " 'had',\n",
              " 'hadn',\n",
              " \"hadn't\",\n",
              " 'has',\n",
              " 'hasn',\n",
              " \"hasn't\",\n",
              " 'have',\n",
              " 'haven',\n",
              " \"haven't\",\n",
              " 'having',\n",
              " 'he',\n",
              " \"he'd\",\n",
              " \"he'll\",\n",
              " 'her',\n",
              " 'here',\n",
              " 'hers',\n",
              " 'herself',\n",
              " \"he's\",\n",
              " 'him',\n",
              " 'himself',\n",
              " 'his',\n",
              " 'how',\n",
              " 'i',\n",
              " \"i'd\",\n",
              " 'if',\n",
              " \"i'll\",\n",
              " \"i'm\",\n",
              " 'in',\n",
              " 'into',\n",
              " 'is',\n",
              " 'isn',\n",
              " \"isn't\",\n",
              " 'it',\n",
              " \"it'd\",\n",
              " \"it'll\",\n",
              " \"it's\",\n",
              " 'its',\n",
              " 'itself',\n",
              " \"i've\",\n",
              " 'just',\n",
              " 'll',\n",
              " 'm',\n",
              " 'ma',\n",
              " 'me',\n",
              " 'mightn',\n",
              " \"mightn't\",\n",
              " 'more',\n",
              " 'most',\n",
              " 'mustn',\n",
              " \"mustn't\",\n",
              " 'my',\n",
              " 'myself',\n",
              " 'needn',\n",
              " \"needn't\",\n",
              " 'no',\n",
              " 'nor',\n",
              " 'not',\n",
              " 'now',\n",
              " 'o',\n",
              " 'of',\n",
              " 'off',\n",
              " 'on',\n",
              " 'once',\n",
              " 'only',\n",
              " 'or',\n",
              " 'other',\n",
              " 'our',\n",
              " 'ours',\n",
              " 'ourselves',\n",
              " 'out',\n",
              " 'over',\n",
              " 'own',\n",
              " 're',\n",
              " 's',\n",
              " 'same',\n",
              " 'shan',\n",
              " \"shan't\",\n",
              " 'she',\n",
              " \"she'd\",\n",
              " \"she'll\",\n",
              " \"she's\",\n",
              " 'should',\n",
              " 'shouldn',\n",
              " \"shouldn't\",\n",
              " \"should've\",\n",
              " 'so',\n",
              " 'some',\n",
              " 'such',\n",
              " 't',\n",
              " 'than',\n",
              " 'that',\n",
              " \"that'll\",\n",
              " 'the',\n",
              " 'their',\n",
              " 'theirs',\n",
              " 'them',\n",
              " 'themselves',\n",
              " 'then',\n",
              " 'there',\n",
              " 'these',\n",
              " 'they',\n",
              " \"they'd\",\n",
              " \"they'll\",\n",
              " \"they're\",\n",
              " \"they've\",\n",
              " 'this',\n",
              " 'those',\n",
              " 'through',\n",
              " 'to',\n",
              " 'too',\n",
              " 'under',\n",
              " 'until',\n",
              " 'up',\n",
              " 've',\n",
              " 'very',\n",
              " 'was',\n",
              " 'wasn',\n",
              " \"wasn't\",\n",
              " 'we',\n",
              " \"we'd\",\n",
              " \"we'll\",\n",
              " \"we're\",\n",
              " 'were',\n",
              " 'weren',\n",
              " \"weren't\",\n",
              " \"we've\",\n",
              " 'what',\n",
              " 'when',\n",
              " 'where',\n",
              " 'which',\n",
              " 'while',\n",
              " 'who',\n",
              " 'whom',\n",
              " 'why',\n",
              " 'will',\n",
              " 'with',\n",
              " 'won',\n",
              " \"won't\",\n",
              " 'wouldn',\n",
              " \"wouldn't\",\n",
              " 'y',\n",
              " 'you',\n",
              " \"you'd\",\n",
              " \"you'll\",\n",
              " 'your',\n",
              " \"you're\",\n",
              " 'yours',\n",
              " 'yourself',\n",
              " 'yourselves',\n",
              " \"you've\"]"
            ]
          },
          "metadata": {},
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#its is available for different lang\n",
        "stopwords.words('german')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zO6UjmVG6PKk",
        "outputId": "03487db4-15b7-4a89-bb4e-c15c14c4197c"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['aber',\n",
              " 'alle',\n",
              " 'allem',\n",
              " 'allen',\n",
              " 'aller',\n",
              " 'alles',\n",
              " 'als',\n",
              " 'also',\n",
              " 'am',\n",
              " 'an',\n",
              " 'ander',\n",
              " 'andere',\n",
              " 'anderem',\n",
              " 'anderen',\n",
              " 'anderer',\n",
              " 'anderes',\n",
              " 'anderm',\n",
              " 'andern',\n",
              " 'anderr',\n",
              " 'anders',\n",
              " 'auch',\n",
              " 'auf',\n",
              " 'aus',\n",
              " 'bei',\n",
              " 'bin',\n",
              " 'bis',\n",
              " 'bist',\n",
              " 'da',\n",
              " 'damit',\n",
              " 'dann',\n",
              " 'der',\n",
              " 'den',\n",
              " 'des',\n",
              " 'dem',\n",
              " 'die',\n",
              " 'das',\n",
              " 'dass',\n",
              " 'daß',\n",
              " 'derselbe',\n",
              " 'derselben',\n",
              " 'denselben',\n",
              " 'desselben',\n",
              " 'demselben',\n",
              " 'dieselbe',\n",
              " 'dieselben',\n",
              " 'dasselbe',\n",
              " 'dazu',\n",
              " 'dein',\n",
              " 'deine',\n",
              " 'deinem',\n",
              " 'deinen',\n",
              " 'deiner',\n",
              " 'deines',\n",
              " 'denn',\n",
              " 'derer',\n",
              " 'dessen',\n",
              " 'dich',\n",
              " 'dir',\n",
              " 'du',\n",
              " 'dies',\n",
              " 'diese',\n",
              " 'diesem',\n",
              " 'diesen',\n",
              " 'dieser',\n",
              " 'dieses',\n",
              " 'doch',\n",
              " 'dort',\n",
              " 'durch',\n",
              " 'ein',\n",
              " 'eine',\n",
              " 'einem',\n",
              " 'einen',\n",
              " 'einer',\n",
              " 'eines',\n",
              " 'einig',\n",
              " 'einige',\n",
              " 'einigem',\n",
              " 'einigen',\n",
              " 'einiger',\n",
              " 'einiges',\n",
              " 'einmal',\n",
              " 'er',\n",
              " 'ihn',\n",
              " 'ihm',\n",
              " 'es',\n",
              " 'etwas',\n",
              " 'euer',\n",
              " 'eure',\n",
              " 'eurem',\n",
              " 'euren',\n",
              " 'eurer',\n",
              " 'eures',\n",
              " 'für',\n",
              " 'gegen',\n",
              " 'gewesen',\n",
              " 'hab',\n",
              " 'habe',\n",
              " 'haben',\n",
              " 'hat',\n",
              " 'hatte',\n",
              " 'hatten',\n",
              " 'hier',\n",
              " 'hin',\n",
              " 'hinter',\n",
              " 'ich',\n",
              " 'mich',\n",
              " 'mir',\n",
              " 'ihr',\n",
              " 'ihre',\n",
              " 'ihrem',\n",
              " 'ihren',\n",
              " 'ihrer',\n",
              " 'ihres',\n",
              " 'euch',\n",
              " 'im',\n",
              " 'in',\n",
              " 'indem',\n",
              " 'ins',\n",
              " 'ist',\n",
              " 'jede',\n",
              " 'jedem',\n",
              " 'jeden',\n",
              " 'jeder',\n",
              " 'jedes',\n",
              " 'jene',\n",
              " 'jenem',\n",
              " 'jenen',\n",
              " 'jener',\n",
              " 'jenes',\n",
              " 'jetzt',\n",
              " 'kann',\n",
              " 'kein',\n",
              " 'keine',\n",
              " 'keinem',\n",
              " 'keinen',\n",
              " 'keiner',\n",
              " 'keines',\n",
              " 'können',\n",
              " 'könnte',\n",
              " 'machen',\n",
              " 'man',\n",
              " 'manche',\n",
              " 'manchem',\n",
              " 'manchen',\n",
              " 'mancher',\n",
              " 'manches',\n",
              " 'mein',\n",
              " 'meine',\n",
              " 'meinem',\n",
              " 'meinen',\n",
              " 'meiner',\n",
              " 'meines',\n",
              " 'mit',\n",
              " 'muss',\n",
              " 'musste',\n",
              " 'nach',\n",
              " 'nicht',\n",
              " 'nichts',\n",
              " 'noch',\n",
              " 'nun',\n",
              " 'nur',\n",
              " 'ob',\n",
              " 'oder',\n",
              " 'ohne',\n",
              " 'sehr',\n",
              " 'sein',\n",
              " 'seine',\n",
              " 'seinem',\n",
              " 'seinen',\n",
              " 'seiner',\n",
              " 'seines',\n",
              " 'selbst',\n",
              " 'sich',\n",
              " 'sie',\n",
              " 'ihnen',\n",
              " 'sind',\n",
              " 'so',\n",
              " 'solche',\n",
              " 'solchem',\n",
              " 'solchen',\n",
              " 'solcher',\n",
              " 'solches',\n",
              " 'soll',\n",
              " 'sollte',\n",
              " 'sondern',\n",
              " 'sonst',\n",
              " 'über',\n",
              " 'um',\n",
              " 'und',\n",
              " 'uns',\n",
              " 'unsere',\n",
              " 'unserem',\n",
              " 'unseren',\n",
              " 'unser',\n",
              " 'unseres',\n",
              " 'unter',\n",
              " 'viel',\n",
              " 'vom',\n",
              " 'von',\n",
              " 'vor',\n",
              " 'während',\n",
              " 'war',\n",
              " 'waren',\n",
              " 'warst',\n",
              " 'was',\n",
              " 'weg',\n",
              " 'weil',\n",
              " 'weiter',\n",
              " 'welche',\n",
              " 'welchem',\n",
              " 'welchen',\n",
              " 'welcher',\n",
              " 'welches',\n",
              " 'wenn',\n",
              " 'werde',\n",
              " 'werden',\n",
              " 'wie',\n",
              " 'wieder',\n",
              " 'will',\n",
              " 'wir',\n",
              " 'wird',\n",
              " 'wirst',\n",
              " 'wo',\n",
              " 'wollen',\n",
              " 'wollte',\n",
              " 'würde',\n",
              " 'würden',\n",
              " 'zu',\n",
              " 'zum',\n",
              " 'zur',\n",
              " 'zwar',\n",
              " 'zwischen']"
            ]
          },
          "metadata": {},
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import PorterStemmer\n",
        "from nltk.tokenize import sent_tokenize\n",
        "stemmer = PorterStemmer()\n",
        "\n",
        "#tokenization process dividing para - sentences\n",
        "nltk.sent_tokenize(paragraph)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Vokz-Pdj6h-g",
        "outputId": "70c7b497-3db1-4baf-8cae-85b35e4f5f31"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['\\nYou are tired, and that is not weakness.',\n",
              " 'It means you’ve been trying.',\n",
              " 'It means you’ve been showing up when it would’ve been easier to disappear.',\n",
              " 'Progress doesn’t always look like results.',\n",
              " 'Sometimes it looks like discipline without applause.',\n",
              " 'There will be days when effort gives nothing back.',\n",
              " 'Days when studying feels heavy and pointless.',\n",
              " 'Days when motivation is silent.',\n",
              " 'On those days, remember this: consistency matters more than intensity.',\n",
              " 'You don’t need to give 100%.',\n",
              " 'You need to give *something*.',\n",
              " 'Even 20 minutes counts.',\n",
              " 'Even one page counts.',\n",
              " 'Even one problem solved slowly counts.',\n",
              " 'Success is built in boring moments.',\n",
              " 'In quiet rooms.',\n",
              " 'In tired eyes.',\n",
              " 'In “I’ll try anyway” energy.',\n",
              " 'You are not behind.',\n",
              " 'You are in training.',\n",
              " 'Every small session is a vote for the person you are becoming.',\n",
              " 'Every time you sit down despite resistance, you win.',\n",
              " 'Results lag behind effort.',\n",
              " 'Always.',\n",
              " 'But they never appear without it.',\n",
              " 'Don’t quit because today was hard.',\n",
              " 'Don’t quit because yesterday was empty.',\n",
              " 'Don’t quit because the future is unclear.',\n",
              " 'Show up.',\n",
              " 'Sit down.',\n",
              " 'Breathe.',\n",
              " 'Do what you can.',\n",
              " '20 minutes today becomes confidence tomorrow.',\n",
              " 'Confidence becomes momentum.',\n",
              " 'Momentum becomes change.',\n",
              " 'You don’t need proof yet.',\n",
              " 'You need patience.',\n",
              " 'Keep going.',\n",
              " 'Not because it’s easy—\\nbut because quitting won’t take you where you want to be.']"
            ]
          },
          "metadata": {},
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#convert it into list\n",
        "sentences = nltk.sent_tokenize(paragraph)"
      ],
      "metadata": {
        "id": "pDe2-f8k600q"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#processed each sentence\n",
        "print(sentences)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I6OeKNiV71LF",
        "outputId": "be79a9a6-7058-4f78-9d4f-d70bfb0ee83b"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['\\nYou are tired, and that is not weakness.', 'It means you’ve been trying.', 'It means you’ve been showing up when it would’ve been easier to disappear.', 'Progress doesn’t always look like results.', 'Sometimes it looks like discipline without applause.', 'There will be days when effort gives nothing back.', 'Days when studying feels heavy and pointless.', 'Days when motivation is silent.', 'On those days, remember this: consistency matters more than intensity.', 'You don’t need to give 100%.', 'You need to give *something*.', 'Even 20 minutes counts.', 'Even one page counts.', 'Even one problem solved slowly counts.', 'Success is built in boring moments.', 'In quiet rooms.', 'In tired eyes.', 'In “I’ll try anyway” energy.', 'You are not behind.', 'You are in training.', 'Every small session is a vote for the person you are becoming.', 'Every time you sit down despite resistance, you win.', 'Results lag behind effort.', 'Always.', 'But they never appear without it.', 'Don’t quit because today was hard.', 'Don’t quit because yesterday was empty.', 'Don’t quit because the future is unclear.', 'Show up.', 'Sit down.', 'Breathe.', 'Do what you can.', '20 minutes today becomes confidence tomorrow.', 'Confidence becomes momentum.', 'Momentum becomes change.', 'You don’t need proof yet.', 'You need patience.', 'Keep going.', 'Not because it ’ s easy— but because quitting won ’ t take you where you want to be .']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# apply stopwords and filter then apply stemming\n",
        "for i in range (len(sentences)):\n",
        "  words = nltk.word_tokenize(sentences[i]) # tokenize words"
      ],
      "metadata": {
        "id": "-XA7oJLR8gjV"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(words) #stopswords"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Qpf97YHG9_22",
        "outputId": "99117681-22b8-493c-a967-f4ba67669294"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Not', 'because', 'it', '’', 's', 'easy—', 'but', 'because', 'quitting', 'won', '’', 't', 'take', 'you', 'where', 'you', 'want', 'to', 'be', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# if not present in stopwords doing stemming\n",
        "words = [stemmer.stem(word) for word in words if word not in set(stopwords.words('english'))]"
      ],
      "metadata": {
        "id": "rt__r_3y-YtC"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# converting all the list of words into sentences (join back to sentence)\n",
        "sentences[i]=' '.join(words)"
      ],
      "metadata": {
        "id": "giZ_x8wiQ2Tr"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sentences"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-n_59l1BkXwQ",
        "outputId": "265f6567-c0e4-4eec-9fa9-dd7e6d9d19c4"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['tire , weak .',\n",
              " 'mean ’ tri .',\n",
              " 'mean ’ show would ’ easier disappear .',\n",
              " 'progress ’ alway look like result .',\n",
              " 'sometim look like disciplin without applaus .',\n",
              " 'day effort give noth back .',\n",
              " 'day studi feel heavi pointless .',\n",
              " 'day motiv silent .',\n",
              " 'day , rememb : consist matter inten .',\n",
              " '’ need give 100 % .',\n",
              " 'need give * someth * .',\n",
              " 'even 20 minut count .',\n",
              " 'even one page count .',\n",
              " 'even one problem solv slowli count .',\n",
              " 'success built bore moment .',\n",
              " 'quiet room .',\n",
              " 'tire eye .',\n",
              " '“ ’ tri anyway ” energi .',\n",
              " 'behind .',\n",
              " 'train .',\n",
              " 'everi small session vote person becom .',\n",
              " 'everi time sit despit resist , win .',\n",
              " 'result lag behind effort .',\n",
              " 'alway .',\n",
              " 'never appear without .',\n",
              " '’ quit today hard .',\n",
              " '’ quit yesterday empti .',\n",
              " '’ quit futur unclear .',\n",
              " 'show .',\n",
              " 'sit .',\n",
              " 'breath .',\n",
              " '.',\n",
              " '20 minut today becom confid tomorrow .',\n",
              " 'confid becom momentum .',\n",
              " 'momentum becom chang .',\n",
              " '’ need proof yet .',\n",
              " 'need patienc .',\n",
              " 'keep go .',\n",
              " '’ easy— quit ’ take want .']"
            ]
          },
          "metadata": {},
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Split sentence into words → word_tokenize().\n",
        "\n",
        "Remove stopwords → keeps only important words.\n",
        "\n",
        "Apply stemming → reduces words to root form.\n",
        "\n",
        "Join words back → get a cleaned, stemmed version of the sentence."
      ],
      "metadata": {
        "id": "PjP6K2QSwwFg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# loop through each sentences\n",
        "#apply tokenization on each sentences -> word\n",
        "# get stopwords\n",
        "# if word not in stopwords list (Only words not in stopwords are kept.)\n",
        "# apply stemming\n",
        "# print sentences\n",
        "from nltk.stem import SnowballStemmer\n",
        "snowballStemmer = SnowballStemmer(\"english\")"
      ],
      "metadata": {
        "id": "Cy1HFmTetzi6"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(len(sentences)):\n",
        "  words = nltk.word_tokenize(sentences[i])\n",
        "  words = [snowballStemmer.stem(word) for word in words if word not in set(stopwords.words('english'))]\n",
        "  sentences[i] = ' '.join(words)"
      ],
      "metadata": {
        "id": "7ax2OtIuvbKo"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sentences"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_aNQYCfsv3Of",
        "outputId": "6d1db364-413c-492c-f26f-38293549a4c5"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['tire , weak .',\n",
              " 'mean ’ tri .',\n",
              " 'mean ’ show would ’ easier disappear .',\n",
              " 'progress ’ alway look like result .',\n",
              " 'sometim look like disciplin without applaus .',\n",
              " 'day effort give noth back .',\n",
              " 'day studi feel heavi pointless .',\n",
              " 'day motiv silent .',\n",
              " 'day , rememb : consist matter inten .',\n",
              " '’ need give 100 % .',\n",
              " 'need give * someth * .',\n",
              " 'even 20 minut count .',\n",
              " 'even one page count .',\n",
              " 'even one problem solv slowli count .',\n",
              " 'success built bore moment .',\n",
              " 'quiet room .',\n",
              " 'tire eye .',\n",
              " '“ ’ tri anyway ” energi .',\n",
              " 'behind .',\n",
              " 'train .',\n",
              " 'everi small session vote person becom .',\n",
              " 'everi time sit despit resist , win .',\n",
              " 'result lag behind effort .',\n",
              " 'alway .',\n",
              " 'never appear without .',\n",
              " '’ quit today hard .',\n",
              " '’ quit yesterday empti .',\n",
              " '’ quit futur unclear .',\n",
              " 'show .',\n",
              " 'sit .',\n",
              " 'breath .',\n",
              " '.',\n",
              " '20 minut today becom confid tomorrow .',\n",
              " 'confid becom momentum .',\n",
              " 'momentum becom chang .',\n",
              " '’ need proof yet .',\n",
              " 'need patienc .',\n",
              " 'keep go .',\n",
              " '’ easy— quit ’ take want .']"
            ]
          },
          "metadata": {},
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#using lemmatization for same\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "lemmatization = WordNetLemmatizer()\n",
        "import nltk\n",
        "nltk.download('wordnet')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "40oj7Uu5w0DR",
        "outputId": "29ac674f-a834-43ea-b45d-63a1c15c81d1"
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 49
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(len(sentences)):\n",
        "  words = nltk.word_tokenize(sentences[i])\n",
        "  words = [lemmatization.lemmatize(word,pos='v') for word in words if word  not in set(stopwords.words('english'))]\n",
        "  sentences[i] = ' '.join(words)"
      ],
      "metadata": {
        "id": "vGvlUUgqymJS"
      },
      "execution_count": 58,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sentences"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n0wO97MXytYx",
        "outputId": "e3947187-d1dd-4ff8-941a-b4b06c238937"
      },
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['tire , weak .',\n",
              " 'mean ’ tri .',\n",
              " 'mean ’ show would ’ easier disappear .',\n",
              " 'progress ’ alway look like result .',\n",
              " 'sometim look like disciplin without applaus .',\n",
              " 'day effort give noth back .',\n",
              " 'day studi feel heavi pointless .',\n",
              " 'day motiv silent .',\n",
              " 'day , rememb : consist matter inten .',\n",
              " '’ need give 100 % .',\n",
              " 'need give * someth * .',\n",
              " 'even 20 minut count .',\n",
              " 'even one page count .',\n",
              " 'even one problem solv slowli count .',\n",
              " 'success build bore moment .',\n",
              " 'quiet room .',\n",
              " 'tire eye .',\n",
              " '“ ’ tri anyway ” energi .',\n",
              " 'behind .',\n",
              " 'train .',\n",
              " 'everi small session vote person becom .',\n",
              " 'everi time sit despit resist , win .',\n",
              " 'result lag behind effort .',\n",
              " 'alway .',\n",
              " 'never appear without .',\n",
              " '’ quit today hard .',\n",
              " '’ quit yesterday empti .',\n",
              " '’ quit futur unclear .',\n",
              " 'show .',\n",
              " 'sit .',\n",
              " 'breath .',\n",
              " '.',\n",
              " '20 minut today becom confid tomorrow .',\n",
              " 'confid becom momentum .',\n",
              " 'momentum becom chang .',\n",
              " '’ need proof yet .',\n",
              " 'need patienc .',\n",
              " 'keep go .',\n",
              " '’ easy— quit ’ take want .']"
            ]
          },
          "metadata": {},
          "execution_count": 59
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Oul-ETRkyzpb"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}